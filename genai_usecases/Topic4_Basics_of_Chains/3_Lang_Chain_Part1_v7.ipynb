{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1zp_cNzv1HT"
      },
      "source": [
        "# Package Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_wDojnx_Kh8"
      },
      "outputs": [],
      "source": [
        "!pip -q install openai langchain huggingface_hub --quiet\n",
        "!pip install cohere --quiet\n",
        "!pip install yfinance --quiet\n",
        "!pip install -U langchain-openai langchain-cohere langchain-huggingface --quiet\n",
        "!pip install pytesseract  --quiet\n",
        "!pip install Pillow --quiet\n",
        "!pip install tesseract --quiet\n",
        "!apt install tesseract-ocr --quiet\n",
        "!apt install libtesseract-dev --quiet\n",
        "!pip install langchain_cohere\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh0jm2L3vsXl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "from langchain_cohere.llms import Cohere\n",
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from langchain_classic.chains import LLMChain, SequentialChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "#from langchain.chains import LLMChain, SequentialChain\n",
        "import yfinance as yf\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "from IPython.display import Image as display_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd6Wk8hYitJz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"openaikey\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"huggingfackey\"\n",
        "os.environ['COHERE_API_KEY'] = \"coherekey\"\n",
        "\n",
        "#Better way\n",
        "#from google.colab import userdata\n",
        "#os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "#os.environ['COHERE_API_KEY'] = userdata.get(\"COHERE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPrFjiRC_-n8"
      },
      "source": [
        "#LLMS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s96e8yceAuF7"
      },
      "source": [
        "## OpenAI model - Paid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulPawj03_9Yp"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_openai=ChatOpenAI(temperature=0.4, max_tokens=15)\n",
        "response = llm_openai.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)\n",
        "\n",
        "# - temperature: Set to 0.9, which controls the randomness of the output.\n",
        "#   A higher temperature results in more varied and unpredictable outputs,\n",
        "#   while a lower temperature produces more deterministic and conservative outputs.\n",
        "#   This is often used in generative tasks to balance between creativity and relevance.\n",
        "\n",
        "# - max_tokens: Set to 256, which specifies the maximum number of tokens (words or pieces of words)\n",
        "#   that the model can generate in a single response.\n",
        "\n",
        "\n",
        "#llm_openai=OpenAI(temperature=0.9, max_tokens=256)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXI9Xg_XLIT7"
      },
      "source": [
        "## Cohere - Opensource Alternative to OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D2zWVYEciad"
      },
      "outputs": [],
      "source": [
        "import langchain_cohere\n",
        "#langchain_cohere.ChatCohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcbo1bDQv9vK"
      },
      "outputs": [],
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "\n",
        "llm = ChatCohere()\n",
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNqbovUnAxAb"
      },
      "source": [
        "## Hugging face model - Free"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REkoa0mqaWuK"
      },
      "outputs": [],
      "source": [
        "!pip3 install langchain-huggingface\n",
        "#!pip3 install huggingface_hub==0.24.0\n",
        "#!pip3 install \"huggingface_hub<0.22\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuvnmxzsAqgE"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "repo_id=\"meta-llama/Meta-Llama-3.1-8B\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=256,   # prefer this over max_length for generation\n",
        ")\n",
        "\n",
        "response = llm.invoke(\"What are some ways to boost creativity?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH61E66eQupw"
      },
      "source": [
        "# Where LLMs fail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rce3pTdxQx2R"
      },
      "outputs": [],
      "source": [
        "llm=OpenAI(temperature=0.7, max_tokens=256)\n",
        "#llm=Cohere(model=\"command-xlarge-nightly\")\n",
        "\n",
        "response = llm.invoke(\"What is current market price of the Apple Stock in 2024?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i17YnregRNlY"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "# Get the current market price of Apple stock\n",
        "apple_stock = yf.Ticker(\"AAPL\")\n",
        "apple_cmp= apple_stock.info[\"currentPrice\"]\n",
        "print(apple_cmp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdjz8TmFDdsg"
      },
      "source": [
        "## Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyJ5AVrxiITF"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(\"Write a 4 line poem on AI\")\n",
        "response1 = llm.invoke(\"Craft a quartet of verses celebrating the marvels of artificial intelligence.\")\n",
        "response2 = llm.invoke(\"Compose a brief, ode to the wonders of AI.\")\n",
        "response3 = llm.invoke(\"Pen a short poem that captures the essence of artificial intelligence.\")\n",
        "response4 = llm.invoke(\"Create a succinct tribute to the advancements in AI.\")\n",
        "\n",
        "print(\"\\n======= response =======\\n\", response)\n",
        "print(\"\\n======= response1 =======\\n\", response1)\n",
        "print(\"\\n======= response2 =======\\n\", response2)\n",
        "print(\"\\n======= response3 =======\\n\", response3)\n",
        "print(\"\\n======= response4 =======\\n\", response4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ktw2SS1Joo2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "\n",
        "template = \"Write a 4 line poem on the subject {subject_name}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"subject_name\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "#print(prompt.format(subject_name=\"Data Science\"))\n",
        "#print(prompt.format(subject_name=\"Fathers Day\"))\n",
        "#print(prompt.format(subject_name=\"Solar System\"))\n",
        "\n",
        "\n",
        "response1 = llm.invoke(prompt.format(subject_name=\"Data Science\"))\n",
        "response2 = llm.invoke(prompt.format(subject_name=\"Fathers Day\"))\n",
        "response3 = llm.invoke(prompt.format(subject_name=\"Solar System\"))\n",
        "print(response1)\n",
        "print(response2)\n",
        "print(response3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOqdJdanJkO2"
      },
      "source": [
        "# LLM Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKTNx5BNAqng"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "\n",
        "from langchain_classic.chains import LLMChain, SequentialChain\n",
        "\n",
        "#llm=OpenAI(temperature=0.1)\n",
        "llm=ChatOpenAI(temperature=0.1)\n",
        "\n",
        "template = \"List down the historically significant steps in the field of {filed_name}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"filed_name\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "chain=LLMChain(llm=llm, prompt=prompt)\n",
        "#chian= prompt | llm\n",
        "result=chain.invoke(\"Machine Learning\")\n",
        "print(result['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYdIJ4unnlaX"
      },
      "source": [
        "### LAB: Example of an LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txWBdo4wnosh"
      },
      "outputs": [],
      "source": [
        "#llm=OpenAI(temperature=0.9)\n",
        "llm=ChatOpenAI(temperature=0.9)\n",
        "\n",
        "template = \"The topic name is {topic}. Explain this topic to a 10 years old kid\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "chain=LLMChain(llm=llm, prompt=prompt)\n",
        "#chian= prompt | llm\n",
        "result=chain.invoke(\"Logistic Regression\")\n",
        "print(result['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpKFwWtcmGEi"
      },
      "source": [
        "# Sequential Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmzHo7gtmgjR"
      },
      "source": [
        "## Chain1 : Finds the top10 books\n",
        "Find out the top ten books on any subject with this dedicated Chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRgEjV60epVK"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y97bObADmLLp"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_classic.chains import LLMChain, SequentialChain\n",
        "#llm=OpenAI(temperature=0.5)\n",
        "llm=ChatOpenAI(temperature=0.5)\n",
        "\n",
        "book_name_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"theme\"],\n",
        "    template=\"\"\"Please provide a simple list of ten well-known\n",
        "                books that center around the theme of {theme}.\n",
        "                Do not include book description\"\"\"\n",
        ")\n",
        "\n",
        "book_name_chain = LLMChain(llm=llm,\n",
        "                           prompt=book_name_prompt_template,\n",
        "                           output_key=\"book_names_list\")\n",
        "#book_name_chain1=prompt | llm | {\"book_names_list\": StrOutputParser()}\n",
        "\n",
        "books_list = book_name_chain.invoke(input=\"personality development\")\n",
        "print(books_list[\"book_names_list\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE6CaPuWen-G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T6S44lJo9Y0"
      },
      "source": [
        "## Chain2 : Gives the summary\n",
        "\n",
        "This delivers a detailed summary for any specified book title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izR7dLL4o_9Z"
      },
      "outputs": [],
      "source": [
        "#llm=OpenAI(temperature=0.9, max_tokens=3000)\n",
        "llm=ChatOpenAI(temperature=0.9, max_tokens=3000)\n",
        "\n",
        "book_summary_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"book_names_list\"],\n",
        "    template=\"\"\"Please take any one book from the books list {book_names_list}.\n",
        "                Mention the book title.\n",
        "                Please provide a comprehensive summary of the book,in three sections\n",
        "                and each section with three summary points\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "book_summary_chain = LLMChain(llm=llm,\n",
        "                              prompt=book_summary_prompt_template,\n",
        "                              output_key=\"book_summary\")\n",
        "\n",
        "book_summary = book_summary_chain.invoke(input=\"The Catcher in the Rye by J.D. Salinger\")\n",
        "\n",
        "# Print the books\n",
        "print(book_summary['book_summary'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MMpptLcp2Mt"
      },
      "source": [
        "## SequentialChain\n",
        "\n",
        "Takes theme as input. It first gets top 10 books from the given theme. Then it provides summary of any one of the top 10 books, without taking an specific input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axEQ8-zkp2bz"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import LLMChain, SequentialChain\n",
        "\n",
        "book_chain = SequentialChain(\n",
        "    chains=[book_name_chain, book_summary_chain],\n",
        "    input_variables=[\"theme\"],\n",
        "    output_variables=[\"book_names_list\", \"book_summary\"]\n",
        "    )\n",
        "\n",
        "# Get the book summary for a specific book based on the theme\n",
        "book_summary = book_chain.invoke(input={\"theme\": \"The Art of Command* by Lincoln Montgomery \"})\n",
        "\n",
        "#print(book_summary)\n",
        "print(book_summary[\"book_summary\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlCAtKkk_4QV"
      },
      "source": [
        "# LAB : Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6WFeafSgU3G"
      },
      "outputs": [],
      "source": [
        "SBIN_Stock_Analysis = \"\"\"\n",
        "\n",
        "Company name is State Bank of India\n",
        "NSE Symbol is SBIN\n",
        "MARKET CAP - ₹ 6,69,078.16 Cr.\n",
        "Company has a good Return on Equity (ROE) track record: 3 Years ROE 13.46%.\n",
        "CASA stands at 42.67% of total deposits.\n",
        "The company has delivered good Profit growth of 51.35% over the past 3 years.\n",
        "Company has delivered good profit growth of 76.1% CAGR over last 5 years.\n",
        "Company has been maintaining a healthy dividend payout of 17.3%.\n",
        "Company's working capital requirements have reduced from 152 days to 118 days\n",
        "The bank has a very low ROA track record. Average ROA of 3 years is 0.70%.\n",
        "Low other Income proportion of 11.03%.High Cost to income ratio of 53.87%.\n",
        "Company has low interest coverage ratio.\n",
        "The company has delivered a poor sales growth of 8.91% over past five years.\n",
        "Company has a low return on equity of 12.8% over last 3 years.\n",
        "Contingent liabilities of Rs.19,00,096 Cr.\n",
        "Company might be capitalizing the interest cost.\n",
        "Earnings include an other income of Rs.1,39,611 Cr.\n",
        "\n",
        "\"\"\"\n",
        "print(SBIN_Stock_Analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8gZP_4cxemV"
      },
      "source": [
        "## Chain1 : Positives and Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8lf7Rlxhy94"
      },
      "outputs": [],
      "source": [
        "#llm=OpenAI(temperature=0, max_tokens=256)\n",
        "llm=ChatOpenAI(temperature=0, max_tokens=256)\n",
        "\n",
        "template =\"\"\"Read the text data from {stock_analysis_input}.\n",
        "              Mention the company name and marekt capital.\n",
        "              Write top3 positive and top3 negative points.\n",
        "              keep the points short\"\"\"\n",
        "\n",
        "information_extraction_prompt = PromptTemplate(\n",
        "    input_variables=[\"stock_analysis_input\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "#print(information_extraction_prompt.format(stock_analysis_input=SBIN_Stock_Analysis))\n",
        "\n",
        "information_extraction_chain=LLMChain( llm=llm,\n",
        "                                       prompt=information_extraction_prompt,\n",
        "                                       output_key=\"Pros_and_Cons\")\n",
        "\n",
        "result=information_extraction_chain.invoke(SBIN_Stock_Analysis)\n",
        "#print(result)\n",
        "#print(result.keys())\n",
        "print(result['Pros_and_Cons'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7bpvCIuzHIW"
      },
      "source": [
        "## Chain2 : Investor Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcz523mpjtTD"
      },
      "outputs": [],
      "source": [
        "#llm=OpenAI(temperature=0, max_tokens=256)\n",
        "llm=ChatOpenAI(temperature=0, max_tokens=256)\n",
        "\n",
        "template =\"\"\"\n",
        "Imagine you've been analyzing stocks for over 15 years.\n",
        "Look at the good and bad points, and see if the company can grow.\n",
        "Right now, is buying shares of this company a smart move?\n",
        "take the data from {Pros_and_Cons}\n",
        "\"\"\"\n",
        "\n",
        "stock_decision_prompt = PromptTemplate(\n",
        "    input_variables=[\"Pros_and_Cons\"],\n",
        "    template=template,\n",
        ")\n",
        "#print(stock_decision_prompt.format(Pros_and_Cons=result['Pros_and_Cons']))\n",
        "\n",
        "stock_decision_chain=LLMChain(llm=llm,\n",
        "                              prompt=stock_decision_prompt,\n",
        "                              output_key=\"Investor_Report\")\n",
        "result=stock_decision_chain.invoke(SBIN_Stock_Analysis)\n",
        "print(result['Investor_Report'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT-GJvJpaWuN"
      },
      "outputs": [],
      "source": [
        "#LLMChain --->\tThe most basic chain. Takes input → formats prompt → calls LLM → returns output.\n",
        "# SequentialChain ---> passes two intermediate outputs and get the final output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgu8E8j7vgJ5"
      },
      "source": [
        "## Final Sequential Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILIEcnASumbC"
      },
      "outputs": [],
      "source": [
        "full_chain=SequentialChain(chains=[information_extraction_chain, stock_decision_chain],\n",
        "                           input_variables=[\"stock_analysis_input\"],\n",
        "                           output_variables=[\"Pros_and_Cons\", \"Investor_Report\"])\n",
        "result=full_chain.invoke(SBIN_Stock_Analysis)\n",
        "print(result[\"Investor_Report\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEg7_WB4vIs4"
      },
      "source": [
        "# LangChain + IDP (Intelligent Document Processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sTIZmVIvoq0"
      },
      "outputs": [],
      "source": [
        "#Get the Images, try Invoice_1.png, Invoice_2.png, Invoice_3.png, Invoice_4.png\n",
        "#Try different images in this example\n",
        "#!wget https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_1.png\n",
        "#!wget https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_2.png\n",
        "#!wget https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_3.png\n",
        "#!wget https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_4.png\n",
        "\n",
        "\n",
        "#image_path=image_path = '/content/Invoice_4.png'\n",
        "#display_image(filename=image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrcWoXm7aWuV"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "\n",
        "# URLs to download\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_1.png\",\n",
        "    \"https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_2.png\",\n",
        "    \"https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_3.png\",\n",
        "    \"https://raw.githubusercontent.com/giridhar276/Datasets/master/IDM_Datasets/Invoices/Invoice_4.png\"\n",
        "]\n",
        "\n",
        "# Download and save to local directory (e.g., current working directory)\n",
        "for url in urls:\n",
        "    filename = url.split(\"/\")[-1]\n",
        "    urllib.request.urlretrieve(url, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQBjc8MyaWuV"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Set image path for Windows (local path)\n",
        "image_path = \"Invoice_4.png\"  # Adjust if stored in a subfolder\n",
        "\n",
        "# Display image\n",
        "def display_image(filename):\n",
        "    image = Image.open(filename)\n",
        "    display(image)\n",
        "\n",
        "display_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdO5cDB6aWuV"
      },
      "outputs": [],
      "source": [
        "!pip3 install pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6LrPNnJUGW"
      },
      "source": [
        "## IDP without LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf7eqPtWJTeg"
      },
      "outputs": [],
      "source": [
        "import pytesseract\n",
        "import re\n",
        "\n",
        "def extract_email_addresses(image_path):\n",
        "    text = pytesseract.image_to_string(image_path)\n",
        "    email_addresses = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
        "    # Regular expression to match dates in DD/MM/YYYY, DD-MM-YYYY, or YYYY-MM-DD formats\n",
        "    dob_patterns = re.findall(r\"\\b(?:\\d{2}[/-]\\d{2}[/-]\\d{4}|\\d{4}[/-]\\d{2}[/-]\\d{2})\\b\", text)\n",
        "    print(\"Email Address: \", email_addresses)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI4GvX5KJaxn"
      },
      "outputs": [],
      "source": [
        "image_path\n",
        "extract_email_addresses(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpRelZc7Kj0h"
      },
      "source": [
        "## IDP with LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0nvlhLEvS9F"
      },
      "outputs": [],
      "source": [
        "#Extract Text from Image\n",
        "img = Image.open(image_path)\n",
        "invoice_text = pytesseract.image_to_string(img)\n",
        "#print(invoice_text)\n",
        "\n",
        "#llm=OpenAI(temperature=0)\n",
        "llm=Cohere(temperature=0)\n",
        "\n",
        "template=\"\"\"\n",
        "Take the information from {invoice_text} and print the itemwise price and quantity.\n",
        "\"\"\"\n",
        "\n",
        "invoice_prompt = PromptTemplate(\n",
        "    input_variables=[\"invoice_text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "invoice_chain=LLMChain(llm=llm, prompt=invoice_prompt, output_key=\"itemwise_price_and_quantity\")\n",
        "result=invoice_chain.invoke(invoice_text)\n",
        "print(result['itemwise_price_and_quantity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPqOGrEVGjAB"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display_markdown\n",
        "\n",
        "result_values=result['itemwise_price_and_quantity']\n",
        "display_markdown(result_values, raw=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWa8CX4Y0cfP"
      },
      "outputs": [],
      "source": [
        "template=\"\"\"\n",
        "Take the information from {invoice_text} and print the client name,phone number, email and total amout\n",
        "\"\"\"\n",
        "\n",
        "invoice_prompt = PromptTemplate(\n",
        "    input_variables=[\"invoice_text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "invoice_chain=LLMChain(llm=llm, prompt=invoice_prompt, output_key=\"contact_details\")\n",
        "result=invoice_chain.invoke(invoice_text)\n",
        "print(result['contact_details'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqBFd1tK2D6W"
      },
      "outputs": [],
      "source": [
        "template=\"\"\"\n",
        "Take the information from {invoice_text} and print the bank account number and payment conditions\n",
        "\"\"\"\n",
        "\n",
        "invoice_prompt = PromptTemplate(\n",
        "    input_variables=[\"invoice_text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "invoice_chain=LLMChain(llm=llm, prompt=invoice_prompt, output_key=\"bank_details\")\n",
        "result=invoice_chain.invoke(invoice_text)\n",
        "print(result['bank_details'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQkMLUrfY3qR"
      },
      "source": [
        "# Assignment - Book Summary App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3vLanZ2Sov6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
