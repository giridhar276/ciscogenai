{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6caded40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded (or already available).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/giridharsripathi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download commonly-used NLTK resources for this notebook.\n",
    "# If you already have them, NLTK will just say \"already up-to-date\".\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "print(\"NLTK resources downloaded (or already available).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7466fa8",
   "metadata": {},
   "source": [
    "## 1) Tokenization (Word Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06277e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: IBM Watson is transforming enterprise AI.\n",
      "Tokens: ['IBM', 'Watson', 'is', 'transforming', 'enterprise', 'AI', '.']\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Break a sentence into word-level tokens (words + punctuation).\n",
    "# Why it matters: Tokenization is the first step for many NLP pipelines (classification, NER, etc.).\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"IBM Watson is transforming enterprise AI.\"\n",
    "# word_tokenize splits text into words and punctuation tokens.\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d15e537",
   "metadata": {},
   "source": [
    "## 2) Sentence Segmentation (Sentence Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13e28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: IBM builds AI. It works on Watson. It helps companies.\n",
      "Sentences: ['IBM builds AI.', 'It works on Watson.', 'It helps companies.']\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Split a paragraph into individual sentences.\n",
    "# Why it matters: Useful for summarization, sentence-level sentiment, sentence embeddings, etc.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"IBM builds AI. It works on Watson. It helps companies.\"\n",
    "# sent_tokenize identifies sentence boundaries (handles common punctuation cases).\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Sentences:\", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaa411",
   "metadata": {},
   "source": [
    "## 3) Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7d5bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['this', 'is', 'a', 'cloud', 'platform']\n",
      "Filtered: ['cloud', 'platform']\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Remove commonly occurring words like \"the\", \"is\", \"a\" that often add little meaning.\n",
    "# Why it matters: Helps reduce noise for tasks like keyword extraction and classical ML models.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "words = [\"this\", \"is\", \"a\", \"cloud\", \"platform\"]\n",
    "stop_words = set(stopwords.words(\"english\"))  # Load stopwords list for English\n",
    "\n",
    "# Keep only words that are NOT stopwords\n",
    "filtered = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "print(\"Original:\", words)\n",
    "print(\"Filtered:\", filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de17475",
   "metadata": {},
   "source": [
    "## 4) Stemming (Porter Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6139de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['running', 'runs', 'ran', 'runner']\n",
      "Stems: ['run', 'run', 'ran', 'runner']\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Reduce words to a crude base form (stem).\n",
    "# Example: \"running\" -> \"run\"\n",
    "# Why it matters: Helps match variants of the same word in search / retrieval / classical models.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"ran\", \"runner\"]\n",
    "\n",
    "# Stem each word (rule-based, not always a real dictionary word)\n",
    "stems = [ps.stem(w) for w in words]\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Stems:\", stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f61cd",
   "metadata": {},
   "source": [
    "## 5) Lemmatization (WordNet Lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37d6b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['better', 'running', 'cars']\n",
      "Lemmas: ['better', 'running', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Convert a word into its dictionary base form (lemma).\n",
    "# Why it matters: Usually cleaner than stemming because it tries to return valid words.\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "words = [\"better\", \"running\", \"cars\"]\n",
    "\n",
    "# Note: Without POS info, lemmatizer assumes noun by default.\n",
    "lemmas = [lemm.lemmatize(w) for w in words]\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Lemmas:\", lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5078c5",
   "metadata": {},
   "source": [
    "## 6) Part-of-Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "085a20c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: IBM Watson builds smart systems\n",
      "POS tagged: [('IBM', 'NNP'), ('Watson', 'NNP'), ('builds', 'VBZ'), ('smart', 'JJ'), ('systems', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Tag each word with its grammatical role (noun, verb, adjective, etc.)\n",
    "# Why it matters: Useful for chunking, parsing, keyword extraction (e.g., nouns), etc.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"IBM Watson builds smart systems\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# pos_tag assigns a POS label to each token\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"POS tagged:\", tagged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3bdd2",
   "metadata": {},
   "source": [
    "## 7) Named Entity Recognition (NER) with Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aab80ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: IBM is located in New York\n",
      "NER tree: (S\n",
      "  (ORGANIZATION IBM/NNP)\n",
      "  is/VBZ\n",
      "  located/VBN\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Detect named entities like organizations, locations, people.\n",
    "# Why it matters: Useful in incident analysis, ticket classification, compliance, and entity indexing.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"IBM is located in New York\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# ne_chunk converts POS-tagged tokens into a tree containing named entity chunks\n",
    "entities_tree = ne_chunk(tagged)\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"NER tree:\", entities_tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5ace1",
   "metadata": {},
   "source": [
    "## 8) Frequency Distribution (Most Common Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224b7e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['AI', 'AI', 'ML', 'ML', 'ML', 'NLP']\n",
      "Most common: [('ML', 3), ('AI', 2), ('NLP', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Count how frequently each token occurs.\n",
    "# Why it matters: Basic analytics for keyword discovery, vocabulary building, etc.\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"AI AI ML ML ML NLP\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "fd = FreqDist(tokens)  # Count token frequencies\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Most common:\", fd.most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9d6d2",
   "metadata": {},
   "source": [
    "## 9) Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa84c7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['machine', 'learning', 'models', 'are', 'powerful']\n",
      "Bigrams: [('machine', 'learning'), ('learning', 'models'), ('models', 'are'), ('are', 'powerful')]\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Generate 2-word sequences (bigrams).\n",
    "# Why it matters: Useful for phrase mining like \"machine learning\", \"data science\", etc.\n",
    "\n",
    "from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"machine learning models are powerful\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# bigrams returns an iterator of consecutive pairs\n",
    "bg = list(bigrams(tokens))\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Bigrams:\", bg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc562470",
   "metadata": {},
   "source": [
    "## 10) Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b696e046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['AI', 'models', 'learn', 'from', 'data']\n",
      "Trigrams: [('AI', 'models', 'learn'), ('models', 'learn', 'from'), ('learn', 'from', 'data')]\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Generate 3-word sequences (trigrams).\n",
    "# Why it matters: Captures context better than bigrams for phrase patterns.\n",
    "\n",
    "from nltk import trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"AI models learn from data\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "tg = list(trigrams(tokens))\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Trigrams:\", tg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85ae68",
   "metadata": {},
   "source": [
    "## 11) Text Cleaning (Regex-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a0201ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: IBM!! Watson?? 2025\n",
      "Cleaned: IBM Watson \n"
     ]
    }
   ],
   "source": [
    "# Use-case: Remove special characters/numbers and keep only letters + spaces.\n",
    "# Why it matters: Preprocessing step for classical NLP models or keyword extraction.\n",
    "\n",
    "import re\n",
    "\n",
    "text = \"IBM!! Watson?? 2025\"\n",
    "# Replace everything that's NOT a letter or space with empty string.\n",
    "clean = re.sub(r\"[^a-zA-Z ]\", \"\", text)\n",
    "\n",
    "# re.sub(source,destination)\n",
    "\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Cleaned:\", clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572de93",
   "metadata": {},
   "source": [
    "## 12) Bag of Words (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6a6301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['ai' 'is' 'powerful' 'smart']\n",
      "BoW matrix:\n",
      " [[1 1 1 0]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Convert a collection of documents into a bag-of-words numeric matrix.\n",
    "# Why it matters: Classical ML models (LogReg, NB, SVM) often use this representation.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\"AI is powerful\", \"AI is smart\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)  # Learn vocab + transform docs\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9b688",
   "metadata": {},
   "source": [
    "## 13) TF-IDF (Term importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a34612f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['ai' 'builds' 'intelligence' 'systems']\n",
      "TF-IDF matrix:\n",
      " [[0.50154891 0.50154891 0.         0.70490949]\n",
      " [0.50154891 0.50154891 0.70490949 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Convert text into TF-IDF features (downweights common words, upweights important words).\n",
    "# Why it matters: Strong baseline for search, similarity, and classical ML.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\"AI builds systems\", \"AI builds intelligence\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(docs)\n",
    "\n",
    "print(\"Features:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b810d",
   "metadata": {},
   "source": [
    "## 14) Simple Text Similarity (Jaccard over token sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca98ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1: AI builds models\n",
      "Text2: AI builds systems\n",
      "Jaccard similarity: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Compute rough similarity between two short texts using token overlap.\n",
    "# Why it matters: Useful as a quick baseline for deduplication / clustering heuristics.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "t1 = \"AI builds models\"\n",
    "t2 = \"AI builds systems\"\n",
    "\n",
    "set1 = set(word_tokenize(t1.lower()))\n",
    "set2 = set(word_tokenize(t2.lower()))\n",
    "\n",
    "# Jaccard similarity = intersection / union\n",
    "similarity = len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "print(\"Text1:\", t1)\n",
    "print(\"Text2:\", t2)\n",
    "print(\"Jaccard similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453bf5a",
   "metadata": {},
   "source": [
    "## 15) Spam Detection (Toy Naive Bayes with NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1171f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test features: {'free': True, 'offer': True}\n",
      "Prediction: spam\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Classify messages as spam/ham using NLTK's NaiveBayesClassifier.\n",
    "# Why it matters: Demonstrates a classic NLP classification approach quickly.\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "# Training data format: (feature_dict, label)\n",
    "# Here we use a very small toy dataset just for demonstration.\n",
    "train = [\n",
    "    ({\"free\": True, \"win\": True}, \"spam\"),\n",
    "    ({\"hello\": True, \"meeting\": True}, \"ham\"),\n",
    "    ({\"free\": True, \"prize\": True}, \"spam\"),\n",
    "    ({\"project\": True, \"update\": True}, \"ham\"),\n",
    "]\n",
    "\n",
    "model = NaiveBayesClassifier.train(train)\n",
    "\n",
    "# Predict a new message represented as features\n",
    "test_features = {\"free\": True, \"offer\": True}\n",
    "prediction = model.classify(test_features)\n",
    "\n",
    "print(\"Test features:\", test_features)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166b651",
   "metadata": {},
   "source": [
    "## 16) Sentiment Analysis (VADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b02e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: IBM Watson is amazing and very helpful.\n",
      "Scores: {'neg': 0.0, 'neu': 0.42, 'pos': 0.58, 'compound': 0.7841}\n"
     ]
    }
   ],
   "source": [
    "# Use-case: Get sentiment scores for a sentence (positive/negative/neutral).\n",
    "# Why it matters: Useful for feedback analysis, survey analytics, social media monitoring.\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "text = \"IBM Watson is amazing and very helpful.\"\n",
    "scores = sia.polarity_scores(text)  # returns dict with pos/neu/neg/compound\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb9c89",
   "metadata": {},
   "source": [
    "## 17) Chunking (Phrase Extraction using POS patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e4c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use-case: Extract noun phrases using a simple chunk grammar.\n",
    "# Why it matters: Quick way to extract \"key phrases\" like 'smart system', 'cloud platform', etc.\n",
    "\n",
    "import nltk\n",
    "from nltk import RegexpParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Grammar: NP (Noun Phrase) = optional determiner + adjectives + noun\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "text = \"The smart system\"\n",
    "tokens = word_tokenize(text)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "tree = parser.parse(tagged)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Chunk tree:\", tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efebeca",
   "metadata": {},
   "source": [
    "## 18) Keyword Extraction (FreqDist + Stopword filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66395a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use-case: Extract top keywords by frequency after basic cleaning.\n",
    "# Why it matters: Quick baseline keyword extraction for tickets / logs / documents.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "\n",
    "text = \"AI models build predictive systems. Predictive systems help enterprises.\"\n",
    "# 1) Lowercase\n",
    "text = text.lower()\n",
    "# 2) Remove non-letters\n",
    "text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "# 3) Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# 4) Remove stopwords + short tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "\n",
    "fd = FreqDist(tokens)\n",
    "print(\"Top keywords:\", fd.most_common(5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
